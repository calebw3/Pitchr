{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pitchr import data\n",
    "from pitchr import xml_parser\n",
    "from pitchr import harmony_maker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Flatten, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "DATA_PATH = \"../dataset/_xml_scores\"\n",
    "score_files = os.listdir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 210\n"
     ]
    }
   ],
   "source": [
    "# gets list of all melody and harmony dfs\n",
    "all_melody_dfs, all_harmony_dfs = xml_parser.get_all_data()\n",
    "print(len(all_melody_dfs), len(all_harmony_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Converting dfs to numpy arrays\n",
    "minimum_melody = 999\n",
    "minimum_harmony = 999\n",
    "all_melody_np = []\n",
    "all_harmony_np = []\n",
    "for df in all_melody_dfs:\n",
    "    df = df[['Pitch Number', 'Pitch Interval', 'Pitch Predictability']]\n",
    "    temp_np = df.to_numpy()\n",
    "    all_melody_np.append(temp_np)\n",
    "for df in all_harmony_dfs:\n",
    "    df = df[['Pitch Number', 'Pitch Interval', 'Pitch Predictability']]\n",
    "    temp_np = df.to_numpy()\n",
    "    all_harmony_np.append(temp_np)\n",
    "\n",
    "\n",
    "# normalize harmony sizes\n",
    "all_harmony_np = xml_parser.prepare_harmony(all_harmony_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframes to numpy arrays\n",
    "all_melody_np = np.asarray(all_melody_np)\n",
    "all_harmony_np = np.asarray(all_harmony_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 50, 3)\n",
      "(210, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "# convert all values to floats\n",
    "# min pitch number is -33\n",
    "# max pitch number is 43\n",
    "for array in all_melody_np:\n",
    "    for note in array:\n",
    "        for i in range(0, 3):\n",
    "            if type(note[i]) == str:\n",
    "                note[i] = float(-50)\n",
    "                np.float32\n",
    "            elif type(note[i]) == int:\n",
    "                note[i] = float(note[i])\n",
    "for array in all_harmony_np:\n",
    "    for note in array:\n",
    "        for i in range(0, 3):\n",
    "            if type(note[i]) == str:\n",
    "                note[i] = float(-50)\n",
    "            elif type(note[i]) == int:\n",
    "                note[i] = float(note[i])\n",
    "\n",
    "\n",
    "print(all_melody_np.shape)\n",
    "print(all_harmony_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = tf.convert_to_tensor(all_melody_np, dtype=tf.float32)\n",
    "Y = tf.convert_to_tensor(all_harmony_np, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(151, 50, 3) (151, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(all_melody_np, all_harmony_np, test_size=0.2)\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, test_size=0.1)\n",
    "print(x_train.shape, y_train.shape)\n",
    "x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 50, 3)\n",
      "(210, 50, 3)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 50, 512)           1056768   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 3,188,995\n",
      "Trainable params: 3,188,995\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 50, 512)           1056768   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 3,188,995\n",
      "Trainable params: 3,188,995\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "168/168 [==============================] - 54s 321ms/step - loss: 292.9747 - val_loss: 438.8478\n",
      "Epoch 2/100\n",
      "168/168 [==============================] - 53s 317ms/step - loss: 292.8891 - val_loss: 438.8510\n",
      "Epoch 3/100\n",
      "168/168 [==============================] - 53s 315ms/step - loss: 292.8661 - val_loss: 438.8403\n",
      "Epoch 4/100\n",
      "168/168 [==============================] - 53s 318ms/step - loss: 292.8560 - val_loss: 438.8394\n",
      "Epoch 5/100\n",
      "168/168 [==============================] - 53s 313ms/step - loss: 292.8654 - val_loss: 438.8493\n",
      "Epoch 6/100\n",
      "168/168 [==============================] - 54s 319ms/step - loss: 292.8625 - val_loss: 438.8397\n",
      "Epoch 7/100\n",
      "168/168 [==============================] - 53s 315ms/step - loss: 292.8599 - val_loss: 438.8405\n",
      "Epoch 8/100\n",
      "168/168 [==============================] - 53s 316ms/step - loss: 292.8618 - val_loss: 438.8395\n",
      "Epoch 9/100\n",
      "168/168 [==============================] - 53s 315ms/step - loss: 292.8595 - val_loss: 438.8427\n",
      "Epoch 10/100\n",
      "168/168 [==============================] - 54s 319ms/step - loss: 292.8586 - val_loss: 438.8419\n",
      "Epoch 11/100\n",
      "168/168 [==============================] - 54s 323ms/step - loss: 292.8597 - val_loss: 438.8394\n",
      "Epoch 12/100\n",
      "168/168 [==============================] - 53s 315ms/step - loss: 292.8595 - val_loss: 438.8395\n",
      "Epoch 13/100\n",
      "168/168 [==============================] - 53s 314ms/step - loss: 292.8691 - val_loss: 438.8411\n",
      "Epoch 14/100\n",
      "168/168 [==============================] - 53s 314ms/step - loss: 292.8586 - val_loss: 438.8412\n",
      "Epoch 15/100\n",
      "168/168 [==============================] - 53s 315ms/step - loss: 292.8605 - val_loss: 438.8406\n",
      "Epoch 16/100\n",
      "168/168 [==============================] - 53s 313ms/step - loss: 292.8575 - val_loss: 438.8466\n",
      "Epoch 17/100\n",
      "168/168 [==============================] - 53s 315ms/step - loss: 292.8588 - val_loss: 438.8402\n",
      "Epoch 18/100\n",
      "168/168 [==============================] - 54s 319ms/step - loss: 292.8580 - val_loss: 438.8395\n",
      "Epoch 19/100\n",
      "168/168 [==============================] - 55s 326ms/step - loss: 292.8585 - val_loss: 438.8437\n",
      "Epoch 20/100\n",
      "168/168 [==============================] - 54s 324ms/step - loss: 292.8602 - val_loss: 438.8462\n",
      "Epoch 21/100\n",
      "168/168 [==============================] - 54s 324ms/step - loss: 292.8595 - val_loss: 438.8410\n",
      "Epoch 22/100\n",
      "168/168 [==============================] - 53s 314ms/step - loss: 292.8588 - val_loss: 438.8404\n",
      "Epoch 23/100\n",
      "168/168 [==============================] - 54s 322ms/step - loss: 292.8593 - val_loss: 438.8397\n",
      "Epoch 24/100\n",
      "168/168 [==============================] - 53s 315ms/step - loss: 292.8595 - val_loss: 438.8419\n",
      "Epoch 25/100\n",
      "168/168 [==============================] - 52s 311ms/step - loss: 292.8580 - val_loss: 438.8397\n",
      "Epoch 26/100\n",
      "168/168 [==============================] - 52s 312ms/step - loss: 292.8647 - val_loss: 438.8404\n",
      "Epoch 27/100\n",
      "168/168 [==============================] - 55s 325ms/step - loss: 292.8592 - val_loss: 438.8411\n",
      "Epoch 28/100\n",
      "168/168 [==============================] - 52s 312ms/step - loss: 292.8588 - val_loss: 438.8418\n",
      "Epoch 29/100\n",
      "168/168 [==============================] - 52s 311ms/step - loss: 292.8617 - val_loss: 438.8398\n",
      "Epoch 30/100\n",
      "168/168 [==============================] - 52s 310ms/step - loss: 293.6976 - val_loss: 438.8895\n",
      "Epoch 31/100\n",
      "168/168 [==============================] - 52s 312ms/step - loss: 292.8691 - val_loss: 438.8396\n",
      "Epoch 32/100\n",
      "168/168 [==============================] - 60s 356ms/step - loss: 292.8622 - val_loss: 438.8435\n",
      "Epoch 33/100\n",
      "168/168 [==============================] - 58s 343ms/step - loss: 292.8591 - val_loss: 438.8396\n",
      "Epoch 34/100\n",
      "168/168 [==============================] - 57s 337ms/step - loss: 292.8589 - val_loss: 438.8431\n",
      "Epoch 35/100\n",
      "168/168 [==============================] - 54s 323ms/step - loss: 292.8617 - val_loss: 438.8406\n",
      "Epoch 36/100\n",
      "168/168 [==============================] - 54s 322ms/step - loss: 292.8584 - val_loss: 438.8396\n",
      "Epoch 37/100\n",
      "168/168 [==============================] - 55s 325ms/step - loss: 292.8592 - val_loss: 438.8406\n",
      "Epoch 38/100\n",
      "168/168 [==============================] - 55s 326ms/step - loss: 292.8583 - val_loss: 438.8424\n",
      "Epoch 39/100\n",
      "168/168 [==============================] - 66s 395ms/step - loss: 292.8579 - val_loss: 438.8432\n",
      "Epoch 40/100\n",
      "168/168 [==============================] - 63s 376ms/step - loss: 292.8586 - val_loss: 438.8399\n",
      "Epoch 41/100\n",
      "168/168 [==============================] - 74s 442ms/step - loss: 292.8586 - val_loss: 438.8398\n",
      "Epoch 42/100\n",
      "168/168 [==============================] - 74s 438ms/step - loss: 292.8583 - val_loss: 438.8397\n",
      "Epoch 43/100\n",
      "168/168 [==============================] - 74s 439ms/step - loss: 292.8580 - val_loss: 438.8400\n",
      "Epoch 44/100\n",
      "168/168 [==============================] - 74s 438ms/step - loss: 292.8594 - val_loss: 438.8417\n",
      "Epoch 45/100\n",
      "168/168 [==============================] - 71s 425ms/step - loss: 292.8581 - val_loss: 438.8400\n",
      "Epoch 46/100\n",
      "168/168 [==============================] - 68s 402ms/step - loss: 292.8584 - val_loss: 438.8404\n",
      "Epoch 47/100\n",
      "168/168 [==============================] - 67s 400ms/step - loss: 292.8582 - val_loss: 438.8418\n",
      "Epoch 48/100\n",
      "168/168 [==============================] - 77s 461ms/step - loss: 292.8583 - val_loss: 438.8452\n",
      "Epoch 49/100\n",
      "168/168 [==============================] - 60s 356ms/step - loss: 292.8575 - val_loss: 438.8397\n",
      "Epoch 50/100\n",
      "168/168 [==============================] - 55s 330ms/step - loss: 292.8572 - val_loss: 438.8405\n",
      "Epoch 51/100\n",
      "168/168 [==============================] - 54s 321ms/step - loss: 292.8580 - val_loss: 438.8398\n",
      "Epoch 52/100\n",
      "168/168 [==============================] - 53s 313ms/step - loss: 292.8585 - val_loss: 438.8398\n",
      "Epoch 53/100\n",
      "168/168 [==============================] - 53s 316ms/step - loss: 292.8577 - val_loss: 438.8474\n",
      "Epoch 54/100\n",
      "168/168 [==============================] - 53s 313ms/step - loss: 292.8579 - val_loss: 438.8396\n",
      "Epoch 55/100\n",
      "168/168 [==============================] - 53s 318ms/step - loss: 292.8577 - val_loss: 438.8400\n",
      "Epoch 56/100\n",
      "168/168 [==============================] - 53s 316ms/step - loss: 292.8580 - val_loss: 438.8412\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168/168 [==============================] - 53s 313ms/step - loss: 292.8581 - val_loss: 438.8410\n",
      "Epoch 58/100\n",
      "168/168 [==============================] - 54s 324ms/step - loss: 292.8580 - val_loss: 438.8431\n",
      "Epoch 59/100\n",
      "168/168 [==============================] - 58s 343ms/step - loss: 292.8579 - val_loss: 438.8395\n",
      "Epoch 60/100\n",
      "168/168 [==============================] - 66s 391ms/step - loss: 292.8564 - val_loss: 438.8445\n",
      "Epoch 61/100\n",
      "168/168 [==============================] - 73s 434ms/step - loss: 292.8584 - val_loss: 438.8404\n",
      "Epoch 62/100\n",
      "168/168 [==============================] - 67s 398ms/step - loss: 292.8571 - val_loss: 438.8416\n",
      "Epoch 63/100\n",
      "168/168 [==============================] - 64s 382ms/step - loss: 292.8580 - val_loss: 438.8442\n",
      "Epoch 64/100\n",
      "168/168 [==============================] - 56s 334ms/step - loss: 292.8577 - val_loss: 438.8406\n",
      "Epoch 65/100\n",
      "168/168 [==============================] - 58s 345ms/step - loss: 292.8576 - val_loss: 438.8404\n",
      "Epoch 66/100\n",
      "168/168 [==============================] - 56s 334ms/step - loss: 292.8721 - val_loss: 438.8397\n",
      "Epoch 67/100\n",
      "168/168 [==============================] - 66s 393ms/step - loss: 292.8592 - val_loss: 438.8394\n",
      "Epoch 68/100\n",
      "168/168 [==============================] - 60s 355ms/step - loss: 292.8591 - val_loss: 438.8417\n",
      "Epoch 69/100\n",
      "168/168 [==============================] - 56s 334ms/step - loss: 292.8589 - val_loss: 438.8430\n",
      "Epoch 70/100\n",
      "168/168 [==============================] - 56s 332ms/step - loss: 292.8624 - val_loss: 438.8408\n",
      "Epoch 71/100\n",
      "168/168 [==============================] - 60s 357ms/step - loss: 292.8702 - val_loss: 438.8404\n",
      "Epoch 72/100\n",
      "168/168 [==============================] - 57s 337ms/step - loss: 292.8594 - val_loss: 438.8393\n",
      "Epoch 73/100\n",
      "168/168 [==============================] - 58s 344ms/step - loss: 292.8589 - val_loss: 438.8399\n",
      "Epoch 74/100\n",
      "168/168 [==============================] - 57s 342ms/step - loss: 292.8586 - val_loss: 438.8408\n",
      "Epoch 75/100\n",
      "168/168 [==============================] - 57s 341ms/step - loss: 292.8582 - val_loss: 438.8443\n",
      "Epoch 76/100\n",
      "168/168 [==============================] - 63s 373ms/step - loss: 292.8576 - val_loss: 438.8404\n",
      "Epoch 77/100\n",
      "168/168 [==============================] - 58s 348ms/step - loss: 292.8585 - val_loss: 438.8403\n",
      "Epoch 78/100\n",
      "168/168 [==============================] - 63s 376ms/step - loss: 292.8574 - val_loss: 438.8423\n",
      "Epoch 79/100\n",
      "168/168 [==============================] - 66s 394ms/step - loss: 292.8581 - val_loss: 438.8396\n",
      "Epoch 80/100\n",
      "168/168 [==============================] - 69s 412ms/step - loss: 292.8577 - val_loss: 438.8410\n",
      "Epoch 81/100\n",
      "168/168 [==============================] - 67s 399ms/step - loss: 292.8578 - val_loss: 438.8401\n",
      "Epoch 82/100\n",
      "168/168 [==============================] - 56s 336ms/step - loss: 292.8588 - val_loss: 438.8412\n",
      "Epoch 83/100\n",
      "168/168 [==============================] - 60s 355ms/step - loss: 292.8664 - val_loss: 438.8406\n",
      "Epoch 84/100\n",
      "168/168 [==============================] - 57s 338ms/step - loss: 292.8608 - val_loss: 438.8409\n",
      "Epoch 85/100\n",
      "168/168 [==============================] - 56s 334ms/step - loss: 292.8582 - val_loss: 438.8442\n",
      "Epoch 86/100\n",
      "168/168 [==============================] - 56s 332ms/step - loss: 292.8585 - val_loss: 438.8430\n",
      "Epoch 87/100\n",
      "168/168 [==============================] - 54s 324ms/step - loss: 292.8591 - val_loss: 438.8409\n",
      "Epoch 88/100\n",
      "168/168 [==============================] - 61s 364ms/step - loss: 292.8581 - val_loss: 438.8417\n",
      "Epoch 89/100\n",
      "168/168 [==============================] - 58s 347ms/step - loss: 292.8576 - val_loss: 438.8410\n",
      "Epoch 90/100\n",
      "168/168 [==============================] - 60s 356ms/step - loss: 292.8576 - val_loss: 438.8396\n",
      "Epoch 91/100\n",
      "168/168 [==============================] - 56s 335ms/step - loss: 292.8573 - val_loss: 438.8407\n",
      "Epoch 92/100\n",
      "168/168 [==============================] - 60s 356ms/step - loss: 292.8573 - val_loss: 438.8409\n",
      "Epoch 93/100\n",
      "168/168 [==============================] - 54s 323ms/step - loss: 292.8568 - val_loss: 438.8490\n",
      "Epoch 94/100\n",
      "168/168 [==============================] - 54s 323ms/step - loss: 292.8612 - val_loss: 438.8412\n",
      "Epoch 95/100\n",
      "168/168 [==============================] - 55s 329ms/step - loss: 292.8578 - val_loss: 438.8419\n",
      "Epoch 96/100\n",
      "168/168 [==============================] - 58s 347ms/step - loss: 292.8582 - val_loss: 438.8411\n",
      "Epoch 97/100\n",
      "168/168 [==============================] - 63s 377ms/step - loss: 292.8573 - val_loss: 438.8411\n",
      "Epoch 98/100\n",
      "168/168 [==============================] - 65s 387ms/step - loss: 292.8578 - val_loss: 438.8395\n",
      "Epoch 99/100\n",
      "168/168 [==============================] - 57s 342ms/step - loss: 292.8615 - val_loss: 438.8422\n",
      "Epoch 100/100\n",
      "168/168 [==============================] - 56s 333ms/step - loss: 292.8590 - val_loss: 438.8448\n"
     ]
    }
   ],
   "source": [
    "print(all_melody_np.shape)\n",
    "print(all_harmony_np.shape)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model = Sequential()\n",
    "# 2 LSTM layers\n",
    "model.add(LSTM(200, input_shape=(50, 3), return_sequences=True))\n",
    "model.add(LSTM(200))\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(3, activation=\"softmax\"))\n",
    "model.summary()\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "print(model.summary())\n",
    "trained_model = model.fit(X, Y, epochs=50, validation_split=0.2, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_8 (LSTM)                (None, 50, 512)           1056768   \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 3,188,995\n",
      "Trainable params: 3,188,995\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# trained model\n",
    "summary = model.summary()\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_24 (LSTM)               (None, 50, 200)           163200    \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 504,403\n",
      "Trainable params: 504,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_24 (LSTM)               (None, 50, 200)           163200    \n",
      "_________________________________________________________________\n",
      "lstm_25 (LSTM)               (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 504,403\n",
      "Trainable params: 504,403\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "120/120 [==============================] - 4s 33ms/step - loss: 341.9691 - val_loss: 272.5745\n",
      "Epoch 2/50\n",
      "120/120 [==============================] - 3s 27ms/step - loss: 341.7822 - val_loss: 272.5749\n",
      "Epoch 3/50\n",
      "120/120 [==============================] - 4s 31ms/step - loss: 341.7820 - val_loss: 272.5749\n",
      "Epoch 4/50\n",
      "120/120 [==============================] - 3s 29ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 5/50\n",
      "120/120 [==============================] - 3s 27ms/step - loss: 341.7821 - val_loss: 272.5749\n",
      "Epoch 6/50\n",
      "120/120 [==============================] - 4s 30ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 7/50\n",
      "120/120 [==============================] - 4s 30ms/step - loss: 341.7820 - val_loss: 272.5749\n",
      "Epoch 8/50\n",
      "120/120 [==============================] - 4s 33ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 9/50\n",
      "120/120 [==============================] - 5s 42ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 10/50\n",
      "120/120 [==============================] - 4s 32ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 11/50\n",
      "120/120 [==============================] - 4s 35ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 12/50\n",
      "120/120 [==============================] - 4s 35ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 13/50\n",
      "120/120 [==============================] - 4s 33ms/step - loss: 341.7820 - val_loss: 272.5749\n",
      "Epoch 14/50\n",
      "120/120 [==============================] - 4s 33ms/step - loss: 341.7820 - val_loss: 272.5749\n",
      "Epoch 15/50\n",
      "120/120 [==============================] - 4s 35ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 16/50\n",
      "120/120 [==============================] - 4s 37ms/step - loss: 341.7820 - val_loss: 272.5749\n",
      "Epoch 17/50\n",
      "120/120 [==============================] - 5s 40ms/step - loss: 341.7817 - val_loss: 272.5749\n",
      "Epoch 18/50\n",
      "120/120 [==============================] - 4s 35ms/step - loss: 341.7834 - val_loss: 272.5749\n",
      "Epoch 19/50\n",
      "120/120 [==============================] - 4s 34ms/step - loss: 341.7820 - val_loss: 272.5749\n",
      "Epoch 20/50\n",
      "120/120 [==============================] - 4s 35ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 21/50\n",
      "120/120 [==============================] - 4s 34ms/step - loss: 341.7820 - val_loss: 272.5749\n",
      "Epoch 22/50\n",
      "120/120 [==============================] - 4s 36ms/step - loss: 341.7818 - val_loss: 272.5749\n",
      "Epoch 23/50\n",
      "120/120 [==============================] - 5s 38ms/step - loss: 341.7818 - val_loss: 272.5749\n",
      "Epoch 24/50\n",
      "120/120 [==============================] - 5s 43ms/step - loss: 341.7820 - val_loss: 272.5749\n",
      "Epoch 25/50\n",
      "120/120 [==============================] - 5s 45ms/step - loss: 341.7818 - val_loss: 272.5749\n",
      "Epoch 26/50\n",
      "120/120 [==============================] - 6s 52ms/step - loss: 341.7820 - val_loss: 272.5749\n",
      "Epoch 27/50\n",
      "120/120 [==============================] - 5s 40ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 28/50\n",
      "120/120 [==============================] - 4s 36ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 29/50\n",
      "120/120 [==============================] - 4s 34ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 30/50\n",
      "120/120 [==============================] - 4s 35ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 31/50\n",
      "120/120 [==============================] - 4s 34ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 32/50\n",
      "120/120 [==============================] - 5s 45ms/step - loss: 341.7820 - val_loss: 272.5749\n",
      "Epoch 33/50\n",
      "120/120 [==============================] - 4s 34ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 34/50\n",
      "120/120 [==============================] - 4s 35ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 35/50\n",
      "120/120 [==============================] - 4s 34ms/step - loss: 341.7818 - val_loss: 272.5749\n",
      "Epoch 36/50\n",
      "120/120 [==============================] - 4s 35ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 37/50\n",
      "120/120 [==============================] - 4s 36ms/step - loss: 341.7818 - val_loss: 272.5749\n",
      "Epoch 38/50\n",
      "120/120 [==============================] - 5s 45ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 39/50\n",
      "120/120 [==============================] - 5s 40ms/step - loss: 341.7820 - val_loss: 272.5749\n",
      "Epoch 40/50\n",
      "120/120 [==============================] - 4s 35ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 41/50\n",
      "120/120 [==============================] - 5s 41ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 42/50\n",
      "120/120 [==============================] - 5s 44ms/step - loss: 341.7819 - val_loss: 272.5749\n",
      "Epoch 43/50\n",
      "120/120 [==============================] - 4s 36ms/step - loss: 341.7820 - val_loss: 272.5749\n",
      "Epoch 44/50\n",
      "120/120 [==============================] - 4s 36ms/step - loss: 341.7820 - val_loss: 272.5749\n",
      "Epoch 45/50\n",
      "120/120 [==============================] - 4s 34ms/step - loss: 341.7818 - val_loss: 272.5749\n",
      "Epoch 46/50\n",
      "120/120 [==============================] - 4s 34ms/step - loss: 341.7820 - val_loss: 272.5749\n",
      "Epoch 47/50\n",
      " 60/120 [==============>...............] - ETA: 1s - loss: 307.1634"
     ]
    }
   ],
   "source": [
    "# model2. Trying out less layers\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model2 = Sequential()\n",
    "# 2 LSTM layers\n",
    "model2.add(LSTM(200, input_shape=(50, 3), return_sequences=True))\n",
    "model2.add(LSTM(200))\n",
    "\n",
    "# Dense layers\n",
    "model2.add(Dense(100, activation=\"relu\"))\n",
    "model2.add(Dropout(0.3))\n",
    "\n",
    "# Output Layer\n",
    "model2.add(Dense(3, activation=\"softmax\"))\n",
    "model2.summary()\n",
    "model2.compile(optimizer=optimizer, loss='mse')\n",
    "print(model2.summary())\n",
    "trained_model2 = model2.fit(x_train, y_train, epochs=50, validation_split=0.2, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50, 512)           1056768   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                3250      \n",
      "=================================================================\n",
      "Total params: 3,192,050\n",
      "Trainable params: 3,192,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = harmony_maker.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# compile model\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}